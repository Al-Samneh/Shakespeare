{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000 # how many iterations to train for\neval_interval = 500 # how often to evaluate the loss\nlearning_rate = 3e-4 # learning rate\nn_head = 6 # number of heads in the multi-head attention\nn_layer = 6 # number of layers in the transformer\ndropout = 0.2 # dropout rate\neval_iters = 200 # how many eval iterations to run\nnum_embd = 384 # embedding dimension\n\n# device configuration\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU device: {torch.cuda.get_device_name()}\")\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n# ------------\n\ntorch.manual_seed(1337) # set the random seed to 1337 so the numbers generaterd are the same every time\n\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt # download the dataset\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars) \n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad() # this means that the gradient is not computed for this function\ndef estimate_loss():\n    out = {} # dictionary to store the losses for train and val\n    model.eval()\n    for split in ['train', 'val']: # loop through train and val\n        losses = torch.zeros(eval_iters) # create a tensor to store the losses\n        for k in range(eval_iters): # loop through the eval iterations\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # list of heads\n        self.proj = nn.Linear(num_embd, num_embd) # linear layer to project the output of the heads to the embedding dimension\n        self.dropout = nn.Dropout(dropout) # dropout layer to prevent overfitting\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedForward(nn.Module): # feed forward network\n    def __init__(self, num_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(num_embd,4*num_embd),\n            nn.ReLU(),\n            nn.Linear(4*num_embd, num_embd),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Block(nn.Module): # transformer block\n    def __init__(self, num_embd, n_head):\n        super().__init__()\n        head_size = num_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedForward(num_embd)        \n        self.ln1 = nn.LayerNorm(num_embd)\n        self.ln2 = nn.LayerNorm(num_embd)\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n\n\n# super simple bigram model (only looks at the last token to predict the next token)\n# Potential upgrade could be taking subword tokens instead of letters\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, num_embd)\n        self.position_embedding_table = nn.Embedding(block_size, num_embd)\n        self.blocks = nn.Sequential(*[Block(num_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(num_embd)\n        self.lm_head = nn.Linear(num_embd, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        B,T = idx.shape\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x)\n        x = self.ln_f(x)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        # if targets is None, we are in the training phase, so we return the logits and the loss\n        if targets is None:\n            loss = None\n        else:\n            # in the evaluation phase, we want to get the loss\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C) # (B*T, C)\n            targets = targets.view(B*T) # (B*T)\n            loss = F.cross_entropy(logits, targets)\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            idx_cond = idx[:, -block_size:]\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nclass Head(nn.Module): # head of the multi-head attention\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(num_embd, head_size, bias=False) # linear layer to project the input to the key\n        self.query = nn.Linear(num_embd, head_size, bias=False) # linear layer to project the input to the query\n        self.value = nn.Linear(num_embd, head_size, bias=False) # linear layer to project the input to the value\n\n        # We utilize the triangular matrix to mask the future tokens, if this was another application, we would not need this as the model should be able to see the future tokens     \n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))   \n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)\n        q = self.query(x)\n        v = self.value(x)\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # We multiply the query and key to get the attention weights, and we divide by the square root of the head size to prevent the attention weights from becoming too large\n        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # We mask the future tokens\n        wei = F.softmax(wei, dim=-1) # We apply the softmax function to the attention weights\n        wei = self.dropout(wei) # We apply the dropout to the attention weights\n        out = wei @ v\n        return out\n\nmodel = BigramLanguageModel(vocab_size) # create the model\nm = model.to(device) # move the model to the device (Graphics Card)\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.9)\n\nprint(\"Starting training...\")\nprint(f\"Using device: {device}\")\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\ngenerated_text = decode(m.generate(context, max_new_tokens=10000)[0].tolist())\n\n# Save the generated text to a file\nwith open('output2.txt', 'w', encoding='utf-8') as f:\n    f.write(generated_text)\n\nprint(f\"Generated {len(generated_text)} characters and saved to output.txt\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-03T17:00:41.070097Z","iopub.execute_input":"2025-08-03T17:00:41.070439Z","iopub.status.idle":"2025-08-03T17:32:24.969786Z","shell.execute_reply.started":"2025-08-03T17:00:41.070414Z","shell.execute_reply":"2025-08-03T17:32:24.968946Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nGPU device: Tesla P100-PCIE-16GB\nGPU memory: 15.9 GB\n--2025-08-03 17:00:41--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt.3’\n\ninput.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n\n2025-08-03 17:00:41 (31.6 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n\nStarting training...\nUsing device: cuda\nstep 0: train loss 4.2849, val loss 4.2823\nstep 500: train loss 2.0119, val loss 2.0984\nstep 1000: train loss 1.5956, val loss 1.7743\nstep 1500: train loss 1.4398, val loss 1.6369\nstep 2000: train loss 1.3404, val loss 1.5664\nstep 2500: train loss 1.2798, val loss 1.5329\nstep 3000: train loss 1.2252, val loss 1.5036\nstep 3500: train loss 1.1840, val loss 1.4878\nstep 4000: train loss 1.1461, val loss 1.4820\nstep 4500: train loss 1.1105, val loss 1.4804\nGenerated 10001 characters and saved to output.txt\n","output_type":"stream"}],"execution_count":4}]}